Namespace(batch_size=500, bce_lambda=1.0, bec_p=0.0, bec_p_dec=0.0, ber_lambda=1.0, block_len=100, block_len_high=200, block_len_low=10, bsc_p=0.0, bsc_p_dec=0.0, channel='awgn', code_rate_k=1, code_rate_n=3, dec_act='linear', dec_kernel_size=5, dec_lr=0.005, dec_num_layer=5, dec_num_unit=50, dec_rnn='gru', decoder='TurboAE_rate3_rnn', demod_lr=0.005, demod_num_layer=1, demod_num_unit=20, dropout=0.0, enc_act='elu', enc_clipping='both', enc_grad_limit=0.01, enc_kernel_size=5, enc_lr=0.005, enc_num_layer=2, enc_num_unit=50, enc_quantize_level=2, enc_rnn='gru', enc_truncate_limit=0, enc_value_limit=1.0, encoder='TurboAE_rate3_cnn', extrinsic=1, focal_alpha=1.0, focal_gamma=0.0, img_size=10, init_nw_weight='default', is_interleave=1, is_k_same_code=False, is_parallel=1, is_same_interleaver=1, is_variable_block_len=False, joint_train=0, k_same_code=2, lambda_maxBCE=0.01, loss='bce', mod_lr=0.005, mod_num_layer=1, mod_num_unit=20, mod_pc='block_power', mod_rate=2, momentum=0.9, no_code_norm=False, no_cuda=False, num_ber_puncture=5, num_block=500, num_epoch=10, num_iter_ft=5, num_iteration=6, num_train_dec=5, num_train_demod=5, num_train_enc=1, num_train_mod=1, optimizer='adam', precompute_norm_stats=False, print_pos_ber=False, print_pos_power=False, print_test_traj=True, radar_power=5.0, radar_prob=0.05, rec_quantize=False, rec_quantize_level=2, rec_quantize_limit=1.0, snr_points=12, snr_test_end=4.0, snr_test_start=-1.5, test_channel_mode='block_norm', test_ratio=1, train_channel_mode='block_norm', train_dec_channel_high=2.0, train_dec_channel_low=-1.5, train_enc_channel_high=2.0, train_enc_channel_low=2.0, vv=5)
using random interleaver [26 86  2 55 75 93 16 73 54 95 53 92 78 13  7 30 22 24 33  8 43 62  3 71
 45 48  6 99 82 76 60 80 90 68 51 27 18 56 63 74  1 61 42 41  4 15 17 40
 38  5 91 59  0 34 28 50 11 35 23 52 10 31 66 57 79 85 32 84 14 89 19 29
 49 97 98 69 20 94 72 77 25 37 81 46 39 65 58 12 88 70 87 36 21 83  9 96
 67 64 47 44] [18 29 64 92 72 87  5 15 12 17 61 76  9 78 80  7 33  6 37 74 79  1 45 28
 60 52 25 39 97 44 16 55 83 49 22 70 47  4 82 94 53 66 26 84 31 63  8 75
 98 57 71 99 86 96 69 24 30 13 40 56 68 95 81 19 38 91 54 32 51 85 11 89
 90 36 65 88 41 14 27 50 20 46 67 35 62  2 59 23 58 43 10  0 73 21 77 42
  3 93 48 34]
Channel_AE(
  (enc): ENC_interCNN(
    (enc_cnn_1): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 50, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_cnn_2): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 50, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_cnn_3): DataParallel(
      (module): SameShapeConv1d(
        (cnns): ModuleList(
          (0): Conv1d(1, 50, kernel_size=(5,), stride=(1,), padding=(2,))
          (1): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,))
        )
      )
    )
    (enc_linear_1): DataParallel(
      (module): Linear(in_features=50, out_features=1, bias=True)
    )
    (enc_linear_2): DataParallel(
      (module): Linear(in_features=50, out_features=1, bias=True)
    )
    (enc_linear_3): DataParallel(
      (module): Linear(in_features=50, out_features=1, bias=True)
    )
    (interleaver): Interleaver()
  )
  (dec): DEC_LargeRNN(
    (interleaver): Interleaver()
    (deinterleaver): DeInterleaver()
    (dropout): Dropout(p=0.0, inplace=False)
    (dec1_rnns): ModuleList(
      (0): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (1): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (2): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (3): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (4): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (5): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
    )
    (dec2_rnns): ModuleList(
      (0): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (1): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (2): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (3): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (4): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
      (5): DataParallel(
        (module): GRU(7, 50, num_layers=2, batch_first=True, bidirectional=True)
      )
    )
    (dec1_outputs): ModuleList(
      (0): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (1): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (2): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (3): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (4): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (5): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
    )
    (dec2_outputs): ModuleList(
      (0): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (1): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (2): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (3): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (4): DataParallel(
        (module): Linear(in_features=100, out_features=5, bias=True)
      )
      (5): DataParallel(
        (module): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
====> Epoch: 1 Average loss: 0.70154804  running time 0.7906053066253662
====> Epoch: 1 Average loss: 0.68192965  running time 0.4587748050689697
====> Epoch: 1 Average loss: 0.65032095  running time 0.45374512672424316
====> Epoch: 1 Average loss: 0.62543899  running time 0.45281362533569336
====> Epoch: 1 Average loss: 0.59077430  running time 0.45677781105041504
====> Epoch: 1 Average loss: 0.54311985  running time 0.4569830894470215
====> Test set BCE loss 0.4926184117794037 Custom Loss 0.4926184117794037 with ber  0.21563999354839325
====> Epoch: 2 Average loss: 0.49387944  running time 0.4398484230041504
====> Epoch: 2 Average loss: 0.47611678  running time 0.45310544967651367
====> Epoch: 2 Average loss: 0.43246436  running time 0.45179104804992676
====> Epoch: 2 Average loss: 0.39585072  running time 0.45277881622314453
====> Epoch: 2 Average loss: 0.34321460  running time 0.4546546936035156
====> Epoch: 2 Average loss: 0.28895375  running time 0.4528825283050537
====> Test set BCE loss 0.19357191026210785 Custom Loss 0.19357191026210785 with ber  0.05178000032901764
====> Epoch: 3 Average loss: 0.19374652  running time 0.43795323371887207
====> Epoch: 3 Average loss: 0.21839398  running time 0.4522359371185303
====> Epoch: 3 Average loss: 0.17671239  running time 0.45211100578308105
====> Epoch: 3 Average loss: 0.16471379  running time 0.45397353172302246
====> Epoch: 3 Average loss: 0.16670734  running time 0.4519166946411133
====> Epoch: 3 Average loss: 0.15311806  running time 0.45604991912841797
====> Test set BCE loss 0.0691285952925682 Custom Loss 0.0691285952925682 with ber  0.024960000067949295
====> Epoch: 4 Average loss: 0.06774865  running time 0.43885254859924316
====> Epoch: 4 Average loss: 0.15422654  running time 0.45525074005126953
====> Epoch: 4 Average loss: 0.14476183  running time 0.455780029296875
====> Epoch: 4 Average loss: 0.15164028  running time 0.45059657096862793
====> Epoch: 4 Average loss: 0.16144621  running time 0.46117115020751953
====> Epoch: 4 Average loss: 0.15889928  running time 0.4536111354827881
====> Test set BCE loss 0.0616002231836319 Custom Loss 0.0616002231836319 with ber  0.02133999951183796
====> Epoch: 5 Average loss: 0.06088036  running time 0.44084954261779785
====> Epoch: 5 Average loss: 0.14451779  running time 0.45467567443847656
====> Epoch: 5 Average loss: 0.15640482  running time 0.45478272438049316
====> Epoch: 5 Average loss: 0.14880624  running time 0.4546024799346924
====> Epoch: 5 Average loss: 0.15036952  running time 0.4549376964569092
====> Epoch: 5 Average loss: 0.14531995  running time 0.45478200912475586
====> Test set BCE loss 0.07098802179098129 Custom Loss 0.07098802179098129 with ber  0.024559998884797096
====> Epoch: 6 Average loss: 0.06973020  running time 0.4383838176727295
====> Epoch: 6 Average loss: 0.14205936  running time 0.4478147029876709
====> Epoch: 6 Average loss: 0.14154458  running time 0.4604771137237549
====> Epoch: 6 Average loss: 0.13896890  running time 0.45424795150756836
====> Epoch: 6 Average loss: 0.14273514  running time 0.4551200866699219
====> Epoch: 6 Average loss: 0.14117049  running time 0.45527052879333496
====> Test set BCE loss 0.07289563119411469 Custom Loss 0.07289563119411469 with ber  0.02199999988079071
====> Epoch: 7 Average loss: 0.07345199  running time 0.43982362747192383
====> Epoch: 7 Average loss: 0.14140780  running time 0.45476365089416504
====> Epoch: 7 Average loss: 0.14524038  running time 0.4607689380645752
====> Epoch: 7 Average loss: 0.13828196  running time 0.4568023681640625
====> Epoch: 7 Average loss: 0.14252087  running time 0.4537851810455322
====> Epoch: 7 Average loss: 0.13919988  running time 0.455014705657959
====> Test set BCE loss 0.07115286588668823 Custom Loss 0.07115286588668823 with ber  0.023339999839663506
====> Epoch: 8 Average loss: 0.06969055  running time 0.44082093238830566
====> Epoch: 8 Average loss: 0.13467926  running time 0.45518040657043457
====> Epoch: 8 Average loss: 0.13443810  running time 0.4553794860839844
====> Epoch: 8 Average loss: 0.13358207  running time 0.4558980464935303
====> Epoch: 8 Average loss: 0.13533068  running time 0.45481014251708984
====> Epoch: 8 Average loss: 0.13110402  running time 0.45480823516845703
====> Test set BCE loss 0.0662253201007843 Custom Loss 0.0662253201007843 with ber  0.02037999965250492
====> Epoch: 9 Average loss: 0.06630049  running time 0.44082117080688477
====> Epoch: 9 Average loss: 0.13049431  running time 0.45481204986572266
====> Epoch: 9 Average loss: 0.12855007  running time 0.4537827968597412
====> Epoch: 9 Average loss: 0.13229403  running time 0.45577383041381836
====> Epoch: 9 Average loss: 0.12729692  running time 0.4557828903198242
====> Epoch: 9 Average loss: 0.13052310  running time 0.45578575134277344
====> Test set BCE loss 0.06181032210588455 Custom Loss 0.06181032210588455 with ber  0.01971999928355217
====> Epoch: 10 Average loss: 0.06280905  running time 0.43982410430908203
====> Epoch: 10 Average loss: 0.12681462  running time 0.4558138847351074
====> Epoch: 10 Average loss: 0.12461745  running time 0.45577216148376465
====> Epoch: 10 Average loss: 0.12369822  running time 0.4550023078918457
====> Epoch: 10 Average loss: 0.12297704  running time 0.4537851810455322
====> Epoch: 10 Average loss: 0.12634620  running time 0.45378589630126953
====> Test set BCE loss 0.05960807576775551 Custom Loss 0.05960807576775551 with ber  0.018640000373125076
test loss trajectory [0.4926184117794037, 0.19357191026210785, 0.0691285952925682, 0.0616002231836319, 0.07098802179098129, 0.07289563119411469, 0.07115286588668823, 0.0662253201007843, 0.06181032210588455, 0.05960807576775551]
test ber trajectory [0.21563999354839325, 0.05178000032901764, 0.024960000067949295, 0.02133999951183796, 0.024559998884797096, 0.02199999988079071, 0.023339999839663506, 0.02037999965250492, 0.01971999928355217, 0.018640000373125076]
total epoch 10
saved model ./tmp/torch_model_356646.pt
SNRS [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
no pos BER specified.
Test SNR -1.5 with ber  0.08803999423980713 with bler 1.0
Punctured Test SNR -1.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR -1.0 with ber  0.07825999706983566 with bler 0.998
Punctured Test SNR -1.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR -0.5 with ber  0.06583999842405319 with bler 1.0
Punctured Test SNR -0.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 0.0 with ber  0.055879998952150345 with bler 0.998
Punctured Test SNR 0.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 0.5 with ber  0.04571999981999397 with bler 0.99
Punctured Test SNR 0.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 1.0 with ber  0.03759999945759773 with bler 0.976
Punctured Test SNR 1.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 1.5 with ber  0.029839999973773956 with bler 0.95
Punctured Test SNR 1.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 2.0 with ber  0.022259999066591263 with bler 0.892
Punctured Test SNR 2.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 2.5 with ber  0.016099998727440834 with bler 0.764
Punctured Test SNR 2.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 3.0 with ber  0.01261999923735857 with bler 0.716
Punctured Test SNR 3.0 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 3.5 with ber  0.009379999712109566 with bler 0.606
Punctured Test SNR 3.5 with ber  0.0 with bler 0.0
no pos BER specified.
Test SNR 4.0 with ber  0.006479999981820583 with bler 0.47
Punctured Test SNR 4.0 with ber  0.0 with bler 0.0
final results on SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.08803999423980713, 0.07825999706983566, 0.06583999842405319, 0.055879998952150345, 0.04571999981999397, 0.03759999945759773, 0.029839999973773956, 0.022259999066591263, 0.016099998727440834, 0.01261999923735857, 0.009379999712109566, 0.006479999981820583]
BLER [1.0, 0.998, 1.0, 0.998, 0.99, 0.976, 0.95, 0.892, 0.764, 0.716, 0.606, 0.47]
final results on punctured SNRs  [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
BER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
BLER [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
encoder power is tensor(1., device='cuda:0')
adjusted SNR should be [-1.4999997446509226, -1.0000000166986343, -0.49999973308696327, -0.0, 0.5000001308463472, 1.0000002900227403, 1.5000000201403676, 2.0000002404171053, 2.5000000877622415, 3.0000002493010487, 3.500000207085638, 3.999999717024358]
